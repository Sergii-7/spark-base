{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame Extra Task (готовий розвʼязок)\n",
    "\n",
    "Вимоги ТЗ:\n",
    "- **тільки DataFrame API** (без SQL-рядків)\n",
    "- локальний Spark `master('local[3]')`\n",
    "- результат кожної задачі відтворюється в ноутбуці через `.show()`\n",
    "\n",
    "Дані очікуються в папці `data/` (можуть бути й у підпапках — ноутбук знайде файли).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session (LOCAL)\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[3]\")\n",
    "    .appName(\"spark-hw\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust repo/data detection\n",
    "cwd = Path.cwd().resolve()\n",
    "repo_root = None\n",
    "for p in [cwd, *cwd.parents]:\n",
    "    if (p / \"data\").exists():\n",
    "        repo_root = p\n",
    "        break\n",
    "if repo_root is None:\n",
    "    repo_root = cwd\n",
    "\n",
    "REPO_ROOT = repo_root\n",
    "DATA_DIR = (REPO_ROOT / \"data\").resolve()\n",
    "\n",
    "print(\"CWD:\", cwd)\n",
    "print(\"Repo:\", REPO_ROOT)\n",
    "print(\"Data dir:\", DATA_DIR, \"exists:\", DATA_DIR.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_file(name: str, base: Path = None) -> Path | None:\n",
    "    \"\"\"Find file under base folder (recursive, case-insensitive).\"\"\"\n",
    "    if base is None:\n",
    "        base = DATA_DIR\n",
    "    base = Path(base)\n",
    "    if not base.exists():\n",
    "        return None\n",
    "\n",
    "    p = base / name\n",
    "    if p.exists():\n",
    "        return p\n",
    "\n",
    "    lower = name.lower()\n",
    "    for x in base.iterdir():\n",
    "        if x.name.lower() == lower:\n",
    "            return x\n",
    "\n",
    "    hits = list(base.rglob(name))\n",
    "    if hits:\n",
    "        return hits[0]\n",
    "\n",
    "    for x in base.rglob(\"*\"):\n",
    "        if x.is_file() and x.name.lower() == lower:\n",
    "            return x\n",
    "    return None\n",
    "\n",
    "def read_csv(name: str):\n",
    "    path = locate_file(name)\n",
    "    if path is None:\n",
    "        raise FileNotFoundError(f\"{name} not found under {DATA_DIR}\")\n",
    "    return spark.read.csv(str(path), header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tables (CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_df = read_csv('actor.csv')\n",
    "address_df = read_csv('address.csv')\n",
    "category_df = read_csv('category.csv')\n",
    "city_df = read_csv('city.csv')\n",
    "country_df = read_csv('country.csv')\n",
    "customer_df = read_csv('customer.csv')\n",
    "film_df = read_csv('film.csv')\n",
    "film_actor_df = read_csv('film_actor.csv')\n",
    "film_category_df = read_csv('film_category.csv')\n",
    "inventory_df = read_csv('inventory.csv')\n",
    "language_df = read_csv('language.csv')\n",
    "payment_df = read_csv('payment.csv')\n",
    "rental_df = read_csv('rental.csv')\n",
    "staff_df = read_csv('staff.csv')\n",
    "store_df = read_csv('store.csv')\n",
    "\n",
    "print('Tables loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнє завдання на тему Spark SQL\n",
    "\n",
    "Задачі з домашнього завдання на SQL потрібно розвʼязати за допомогою Spark SQL DataFrame API.\n",
    "\n",
    "- Дампи таблиць знаходяться в папці `data`.\n",
    "- Розвʼязок кожної задачі має бути відображений в самому файлі (використати метод `.show()`).\n",
    "- Використовувати SQL-рядки заборонено — **тільки DataFrame API**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Вивести кількість фільмів в кожній категорії. Результат відсортувати за спаданням.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1 = (\n",
    "    film_category_df\n",
    "    .join(category_df, on='category_id', how='inner')\n",
    "    .groupBy('name')\n",
    "    .agg(F.countDistinct('film_id').alias('films_cnt'))\n",
    "    .orderBy(F.desc('films_cnt'))\n",
    ")\n",
    "task1.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вивести 10 акторів, чиї фільми брали на прокат найбільше. Результат відсортувати за спаданням.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task2 = (\n",
    "    rental_df\n",
    "    .join(inventory_df.select('inventory_id', 'film_id'), on='inventory_id', how='inner')\n",
    "    .join(film_actor_df, on='film_id', how='inner')\n",
    "    .join(actor_df, on='actor_id', how='inner')\n",
    "    .groupBy('actor_id', 'first_name', 'last_name')\n",
    "    .agg(F.count('*').alias('rentals_cnt'))\n",
    "    .orderBy(F.desc('rentals_cnt'))\n",
    "    .limit(10)\n",
    ")\n",
    "task2.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Вивести категорію фільмів, на яку було витрачено найбільше грошей в прокаті\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task3 = (\n",
    "    payment_df\n",
    "    .join(rental_df.select('rental_id', 'inventory_id'), on='rental_id', how='inner')\n",
    "    .join(inventory_df.select('inventory_id', 'film_id'), on='inventory_id', how='inner')\n",
    "    .join(film_category_df, on='film_id', how='inner')\n",
    "    .join(category_df, on='category_id', how='inner')\n",
    "    .groupBy('name')\n",
    "    .agg(F.sum('amount').alias('total_amount'))\n",
    "    .orderBy(F.desc('total_amount'))\n",
    "    .limit(1)\n",
    ")\n",
    "task3.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Вивести назви фільмів, яких не має в inventory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task4 = (\n",
    "    film_df\n",
    "    .select('film_id', 'title')\n",
    "    .join(inventory_df.select('film_id').distinct(), on='film_id', how='left_anti')\n",
    "    .orderBy('title')\n",
    ")\n",
    "task4.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Вивести топ 3 актори, які найбільше зʼявлялись в категорії фільмів “Children”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_cat = category_df.filter(F.col('name') == F.lit('Children')).select('category_id')\n",
    "\n",
    "task5 = (\n",
    "    film_category_df\n",
    "    .join(children_cat, on='category_id', how='inner')\n",
    "    .join(film_actor_df, on='film_id', how='inner')\n",
    "    .join(actor_df, on='actor_id', how='inner')\n",
    "    .groupBy('actor_id', 'first_name', 'last_name')\n",
    "    .agg(F.countDistinct('film_id').alias('films_in_children'))\n",
    "    .orderBy(F.desc('films_in_children'))\n",
    "    .limit(3)\n",
    ")\n",
    "task5.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Spark session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print('Spark stopped')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}