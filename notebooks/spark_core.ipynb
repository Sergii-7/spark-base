{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b0c956",
   "metadata": {},
   "source": [
    "# Spark Core — базові приклади (самодостатній ноутбук)\n",
    "\n",
    "Цей ноутбук демонструє базові операції з **PySpark**: створення `SparkSession`, роботу з DataFrame (CSV/JSON/Parquet/Text), RDD, простий `word count`, а також шаблони для Kafka та JDBC.\n",
    "\n",
    "> Ноутбук зроблений так, щоб **працювати без зовнішніх файлів**: ми спочатку створюємо прикладові дані й записуємо їх у тимчасову папку, а потім зчитуємо назад.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec58db",
   "metadata": {},
   "source": [
    "## 1) Ініціалізація Spark\n",
    "\n",
    "За замовчуванням використовується `local[*]`. Якщо у вас кластер, змініть `master` (наприклад, `spark://spark-master:7077`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType, ArrayType\n",
    ")\n",
    "\n",
    "# Налаштування Spark\n",
    "MASTER = \"local[*]\"  # або \"spark://spark-master:7077\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"spark_core_basics\")\n",
    "    .master(MASTER)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d09977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переконаємося, що Spark працює\n",
    "spark.range(5).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a570c51",
   "metadata": {},
   "source": [
    "## 2) Підготовка демо-даних\n",
    "\n",
    "Створимо невеликий DataFrame і запишемо його у різні формати у `/tmp/spark_core_demo`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2562e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, json, pathlib\n",
    "\n",
    "base_dir = pathlib.Path(\"/tmp/spark_core_demo\")\n",
    "if base_dir.exists():\n",
    "    shutil.rmtree(base_dir)\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "people = spark.createDataFrame(\n",
    "    [\n",
    "        (\"Alice\", 30, 3500.5, [\"python\", \"spark\"]),\n",
    "        (\"Bob\", 24, 2200.0, [\"js\", \"react\"]),\n",
    "        (\"Charlie\", 41, 7800.0, [\"sql\", \"spark\", \"etl\"]),\n",
    "        (\"Daria\", 35, 6100.3, [\"ml\", \"python\"]),\n",
    "    ],\n",
    "    schema=[\"name\", \"age\", \"salary\", \"skills\"],\n",
    ")\n",
    "\n",
    "people.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd2e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запис у CSV / JSON / Parquet\n",
    "csv_path = str(base_dir / \"people_csv\")\n",
    "json_path = str(base_dir / \"people_json\")\n",
    "parquet_path = str(base_dir / \"people_parquet\")\n",
    "\n",
    "(people\n",
    " .withColumn(\"skills\", F.concat_ws(\",\", F.col(\"skills\")))  # CSV з масивом незручно\n",
    " .write.mode(\"overwrite\").option(\"header\", True).csv(csv_path)\n",
    ")\n",
    "\n",
    "people.write.mode(\"overwrite\").json(json_path)\n",
    "people.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "\n",
    "print(\"Written to:\", csv_path, json_path, parquet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297bf0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запис у Text (один рядок — один запис)\n",
    "text_path = str(base_dir / \"android_log_text\")\n",
    "text_df = spark.createDataFrame(\n",
    "    [\n",
    "        (\"WindowManager: Something happened\",),\n",
    "        (\"ActivityManager: Start proc 1234\",),\n",
    "        (\"WindowManager: Another event\",),\n",
    "        (\"SystemServer: Boot completed\",),\n",
    "    ],\n",
    "    [\"line\"]\n",
    ")\n",
    "\n",
    "text_df.write.mode(\"overwrite\").text(text_path)\n",
    "print(\"Written to:\", text_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954127c",
   "metadata": {},
   "source": [
    "## 3) Зчитування CSV у DataFrame\n",
    "\n",
    "### 3.1) `inferSchema` + заголовки\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e98f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(csv_path)\n",
    ")\n",
    "\n",
    "df_csv.printSchema()\n",
    "df_csv.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a537733",
   "metadata": {},
   "source": [
    "### 3.2) Власна схема (StructType)\n",
    "\n",
    "Корисно, коли важлива точність типів або `inferSchema` занадто повільний.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1689a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"skills\", StringType(), True),\n",
    "])\n",
    "\n",
    "df_csv_schema = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .schema(csv_schema)\n",
    "    .csv(csv_path)\n",
    ")\n",
    "\n",
    "df_csv_schema.printSchema()\n",
    "df_csv_schema.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23bfb48",
   "metadata": {},
   "source": [
    "## 4) Зчитування JSON\n",
    "\n",
    "### 4.1) Звичайний JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838db3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.json(json_path)\n",
    "df_json.printSchema()\n",
    "df_json.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c6b88a",
   "metadata": {},
   "source": [
    "### 4.2) Приклад вкладеного JSON\n",
    "\n",
    "Створимо файл зі вкладеним полем і покажемо `explode`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c3d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_path = str(base_dir / \"nested_sales_json\")\n",
    "\n",
    "nested = spark.createDataFrame(\n",
    "    [\n",
    "        (\"order-1\", [{\"sku\": \"A1\", \"qty\": 2}, {\"sku\": \"B7\", \"qty\": 1}]),\n",
    "        (\"order-2\", [{\"sku\": \"A1\", \"qty\": 1}]),\n",
    "    ],\n",
    "    schema=StructType([\n",
    "        StructField(\"order_id\", StringType(), True),\n",
    "        StructField(\"items\", ArrayType(StructType([\n",
    "            StructField(\"sku\", StringType(), True),\n",
    "            StructField(\"qty\", IntegerType(), True),\n",
    "        ])), True),\n",
    "    ])\n",
    ")\n",
    "\n",
    "nested.write.mode(\"overwrite\").json(nested_path)\n",
    "\n",
    "df_nested = spark.read.json(nested_path)\n",
    "df_nested.printSchema()\n",
    "df_nested.show(truncate=False)\n",
    "\n",
    "df_items = (\n",
    "    df_nested\n",
    "    .select(\"order_id\", F.explode(\"items\").alias(\"item\"))\n",
    "    .select(\"order_id\", F.col(\"item.sku\").alias(\"sku\"), F.col(\"item.qty\").alias(\"qty\"))\n",
    ")\n",
    "\n",
    "df_items.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba987413",
   "metadata": {},
   "source": [
    "## 5) Зчитування Parquet\n",
    "\n",
    "Parquet зберігає схему та типи — зазвичай це найкращий формат для Data Lake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c1fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(parquet_path)\n",
    "df_parquet.printSchema()\n",
    "df_parquet.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfd3fd8",
   "metadata": {},
   "source": [
    "## 6) Зчитування Text та базові RDD-операції"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d07496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = spark.read.text(text_path).toDF(\"line\")\n",
    "df_text.show(truncate=False)\n",
    "\n",
    "rdd = df_text.rdd.map(lambda row: row[\"line\"])\n",
    "rdd.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приклади RDD\n",
    "line_count = rdd.count()\n",
    "first_line = rdd.first()\n",
    "filtered = rdd.filter(lambda line: \"WindowManager:\" in line)\n",
    "\n",
    "(line_count, first_line, filtered.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe9e53",
   "metadata": {},
   "source": [
    "## 7) Word Count (RDD)\n",
    "\n",
    "Класика: розбити на слова → порахувати частоти → топ-N.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f4c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "word_counts = (\n",
    "    rdd\n",
    "    .flatMap(lambda line: re.findall(r\"[A-Za-z0-9_]+\", line.lower()))\n",
    "    .map(lambda w: (w, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    ")\n",
    "\n",
    "word_counts.take(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3224936",
   "metadata": {},
   "source": [
    "## 8) Невеликий приклад з DataFrame API\n",
    "\n",
    "Групування + агрегати.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68481c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Топ навичок (з DataFrame)\n",
    "skills_df = (\n",
    "    people\n",
    "    .select(\"name\", F.explode(\"skills\").alias(\"skill\"))\n",
    ")\n",
    "\n",
    "skills_df.groupBy(\"skill\").agg(\n",
    "    F.count(\"*\").alias(\"cnt\"),\n",
    "    F.collect_list(\"name\").alias(\"people\")\n",
    ").orderBy(F.desc(\"cnt\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236ace80",
   "metadata": {},
   "source": [
    "## 9) Шаблон: читання з Kafka (приклад)\n",
    "\n",
    "> Потрібен пакет `org.apache.spark:spark-sql-kafka-0-10_2.12:<spark_version>`.\n",
    "> У реальному оточенні додається через `--packages ...` або `spark.jars.packages`.\n",
    "\n",
    "Цей блок — **шаблон**, він не запускається автоматично.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87629c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ПРИКЛАД (НЕ ЗАПУСКАТИ без налаштувань Kafka):\n",
    "# kafka_df = (\n",
    "#     spark.read.format(\"kafka\")\n",
    "#     .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "#     .option(\"subscribe\", \"my-topic\")\n",
    "#     .option(\"startingOffsets\", \"earliest\")\n",
    "#     .load()\n",
    "# )\n",
    "#\n",
    "# messages = kafka_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\")\n",
    "# messages.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77657574",
   "metadata": {},
   "source": [
    "## 10) Шаблон: JDBC з partitioning (приклад)\n",
    "\n",
    "Це пришвидшує читання великих таблиць.\n",
    "\n",
    "Цей блок — **шаблон**, він не запускається автоматично.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4347c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ПРИКЛАД (НЕ ЗАПУСКАТИ без драйвера та доступу до БД):\n",
    "# jdbc_url = \"jdbc:postgresql://localhost:5432/mydb\"\n",
    "# props = {\"user\": \"user\", \"password\": \"pass\", \"driver\": \"org.postgresql.Driver\"}\n",
    "#\n",
    "# df_jdbc = (\n",
    "#     spark.read.jdbc(\n",
    "#         url=jdbc_url,\n",
    "#         table=\"public.big_table\",\n",
    "#         column=\"id\",      # колонка для розбиття\n",
    "#         lowerBound=1,\n",
    "#         upperBound=10_000_000,\n",
    "#         numPartitions=16,\n",
    "#         properties=props\n",
    "#     )\n",
    "# )\n",
    "# df_jdbc.select(\"id\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754f769e",
   "metadata": {},
   "source": [
    "## 11) Завершення"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97671677",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
